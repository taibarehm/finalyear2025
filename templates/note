# **1. Install packages**

%pip install catboost skimpy

# **2. Load and Prepare the Data**

import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming the data is in a CSV file named 'data_stress.csv'
# Replace the file path with your actual file path
data = pd.read_csv("data_stress.csv")

# Clean column names by stripping whitespace and replacing spaces with underscores
data.columns = data.columns.str.strip()
data.columns = data.columns.str.replace(" ", "_")

# Display the first few rows of the data with cleaned column names
print("Original Data with Cleaned Columns:")
print(data.head())


# **3. Data Preprocessing**

# Handle missing values
# The skimpy summary showed missing values in several columns.
# We will fill numerical missing values with the mean of their respective columns.
for col in data.columns:
    if data[col].dtype in ['float64', 'int64']:
        data[col] = data[col].fillna(data[col].mean())

print("\nMissing values after imputation:")
print(data.isnull().sum())

# Handle outliers (as suggested by the `skimpy` summary)
# We will cap the outliers using the 99th percentile to avoid extreme values.
for col in ['body_temperature', 'blood_oxygen', 'eye_movement', 'heart_rate']:
    upper_limit = data[col].quantile(0.99)
    data[col] = data[col].clip(upper=upper_limit)

print("\nData after capping outliers:")
print(data.describe())

# Separate features (X) and target (y)
X = data.drop('Stress_Levels', axis=1)
y = data['Stress_Levels']

# Feature Scaling using StandardScaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

print("\nFeatures after scaling:")
print(X_scaled_df.head())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42)

print("\nShape of training and testing data:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Optional: Save the preprocessed data to a new CSV file
# This is useful for future use without having to re-run the preprocessing steps.
# preprocessed_data = pd.DataFrame(X_scaled, columns=X.columns)
# preprocessed_data['Stress_Levels'] = y.reset_index(drop=True)
# preprocessed_data.to_csv('preprocessed_stress_data.csv', index=False)
# print("\nPreprocessed data saved to 'preprocessed_stress_data.csv'")

# **4. Model Building (Placeholder)**
# The next steps would involve training your model with the preprocessed data (X_train, y_train)
# and evaluating it with the test data (X_test, y_test).
# The code for CatBoost, LightGBM, and XGBoost is already in the notebook and can be used with the new data splits.